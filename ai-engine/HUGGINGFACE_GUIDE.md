# HuggingFace Model KullanÄ±m Rehberi

Bu rehber, MindCubes AI Engine'de HuggingFace modellerinin nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± aÃ§Ä±klar.

## ğŸ“‹ Ä°Ã§indekiler

1. [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#hÄ±zlÄ±-baÅŸlangÄ±Ã§)
2. [Model SeÃ§imi](#model-seÃ§imi)
3. [Quantization (Bellek Optimizasyonu)](#quantization-bellek-optimizasyonu)
4. [Gated Models](#gated-models-Ã¶zel-eriÅŸim)
5. [Ã–nerilen Modeller](#Ã¶nerilen-modeller)
6. [Performans Ä°puÃ§larÄ±](#performans-iÌ‡puÃ§larÄ±)
7. [Sorun Giderme](#sorun-giderme)

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Temel KullanÄ±m

```python
from core import LocalModelProvider

# Model oluÅŸtur (otomatik indirilir)
provider = LocalModelProvider(
    model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    device="auto",
    cache_dir="./models/cache"
)

# Metin Ã¼ret
response = await provider.generate(
    prompt="Python'da liste nasÄ±l oluÅŸturulur?",
    system_prompt="Sen yardÄ±mcÄ± bir asistansÄ±n."
)

print(response)
```

### Agent ile KullanÄ±m

```python
from core import LocalModelProvider
from agents import CodeAgent

# LLM provider oluÅŸtur
llm = LocalModelProvider(
    model_name="codellama/CodeLlama-7b-Instruct-hf",
    load_in_4bit=True  # Bellek tasarrufu
)

# Agent oluÅŸtur
agent = CodeAgent(llm_provider=llm)

# Kullan
response = await agent.process("Fibonacci fonksiyonu yaz")
```

---

## ğŸ¯ Model SeÃ§imi

### GPU BelleÄŸine GÃ¶re Model SeÃ§imi

| GPU Bellek | Ã–nerilen Model Boyutu | Quantization |
|------------|----------------------|--------------|
| < 4GB      | 1-2B parametreli     | 4-bit        |
| 4-8GB      | 2-7B parametreli     | 4-bit        |
| 8-16GB     | 7-13B parametreli    | 8-bit        |
| > 16GB     | 13B+ parametreli     | Opsiyonel    |

### Model BoyutlarÄ±

```python
# KÃ¼Ã§Ã¼k Modeller (< 2GB)
"TinyLlama/TinyLlama-1.1B-Chat-v1.0"      # 1.1B - ~1GB
"stabilityai/stablelm-2-1_6b"             # 1.6B - ~1.5GB

# Orta Modeller (2-4GB)
"microsoft/phi-2"                          # 2.7B - ~2.5GB
"Salesforce/codegen-2B-mono"              # 2B - ~2GB

# BÃ¼yÃ¼k Modeller (> 4GB)
"codellama/CodeLlama-7b-Instruct-hf"      # 7B - ~13GB
"meta-llama/Llama-2-7b-chat-hf"           # 7B - ~13GB
"mistralai/Mistral-7B-Instruct-v0.2"      # 7B - ~13GB
```

---

## ğŸ’¾ Quantization (Bellek Optimizasyonu)

### 4-bit Quantization (En DÃ¼ÅŸÃ¼k Bellek)

**Avantajlar:**
- 75% daha az bellek kullanÄ±mÄ±
- 7B model ~3-4GB GPU memory
- HÄ±z kaybÄ± minimal

**Dezavantajlar:**
- Hafif kalite kaybÄ± (~2-3%)
- bitsandbytes kÃ¼tÃ¼phanesi gerekli

```python
provider = LocalModelProvider(
    model_name="codellama/CodeLlama-7b-Instruct-hf",
    load_in_4bit=True,  # 4-bit quantization
    device="auto"
)
```

### 8-bit Quantization (Orta Yol)

**Avantajlar:**
- 50% daha az bellek kullanÄ±mÄ±
- Minimal kalite kaybÄ± (~1%)
- Ä°yi hÄ±z

**Dezavantajlar:**
- 4-bit'ten daha fazla bellek

```python
provider = LocalModelProvider(
    model_name="meta-llama/Llama-2-7b-chat-hf",
    load_in_8bit=True,  # 8-bit quantization
    device="auto"
)
```

### Normal (Float16)

**Avantajlar:**
- En iyi kalite
- Daha hÄ±zlÄ± inference

**Dezavantajlar:**
- En fazla bellek kullanÄ±mÄ±

```python
provider = LocalModelProvider(
    model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    device="auto"  # Normal float16
)
```

---

## ğŸ” Gated Models (Ã–zel EriÅŸim)

BazÄ± modeller (Llama-2, Llama-3, vb.) HuggingFace'te "gated" durumdadÄ±r. Bu modellere eriÅŸim iÃ§in:

### AdÄ±m 1: Model EriÅŸimi Talep Et

1. Model sayfasÄ±na git (Ã¶rn: `meta-llama/Llama-2-7b-chat-hf`)
2. "Request Access" butonuna tÄ±kla
3. Formu doldur ve gÃ¶nder
4. Onay bekle (genellikle birkaÃ§ dakika)

### AdÄ±m 2: HuggingFace Token OluÅŸtur

1. https://huggingface.co/settings/tokens adresine git
2. "New token" oluÅŸtur
3. Token'Ä± kopyala

### AdÄ±m 3: Token'Ä± Ayarla

**Linux/macOS:**
```bash
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxx"
```

**Windows (PowerShell):**
```powershell
$env:HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxx"
```

**Veya .env dosyasÄ±na ekle:**
```bash
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

### AdÄ±m 4: Kullan

```python
provider = LocalModelProvider(
    model_name="meta-llama/Llama-2-7b-chat-hf",
    hf_token=os.getenv("HF_TOKEN"),  # Token otomatik okunur
    load_in_4bit=True
)
```

---

## ğŸŒŸ Ã–nerilen Modeller

### Kod Ãœretimi Ä°Ã§in

#### 1. **CodeLlama-7b-Instruct** (Ã–NERÄ°LÄ°R)
```python
model_name="codellama/CodeLlama-7b-Instruct-hf"
```
- **Boyut:** 7B parametreli (~13GB, 4-bit ile ~4GB)
- **GÃ¼Ã§lÃ¼:** Python, JavaScript, Java, C++
- **Ã–zellik:** Kod tamamlama, debugging, aÃ§Ä±klama

#### 2. **StarCoder**
```python
model_name="bigcode/starcoder"
```
- **Boyut:** 15B parametreli
- **GÃ¼Ã§lÃ¼:** 80+ programlama dili
- **Ã–zellik:** GeniÅŸ dil desteÄŸi

#### 3. **CodeGen-2B** (KÃ¼Ã§Ã¼k GPU iÃ§in)
```python
model_name="Salesforce/codegen-2B-mono"
```
- **Boyut:** 2B parametreli (~2GB)
- **GÃ¼Ã§lÃ¼:** Python
- **Avantaj:** DÃ¼ÅŸÃ¼k bellek gereksinimi

### Genel Chat Ä°Ã§in

#### 1. **Phi-2** (Ã–NERÄ°LÄ°R - KÃ¼Ã§Ã¼k GPU)
```python
model_name="microsoft/phi-2"
```
- **Boyut:** 2.7B parametreli (~2.5GB)
- **Avantaj:** KÃ¼Ã§Ã¼k ama gÃ¼Ã§lÃ¼
- **GÃ¼Ã§lÃ¼:** Reasoning, kod, matematik

#### 2. **TinyLlama** (En KÃ¼Ã§Ã¼k)
```python
model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
```
- **Boyut:** 1.1B parametreli (~1GB)
- **Avantaj:** Ã‡ok hÄ±zlÄ±, Ã§ok az bellek
- **KullanÄ±m:** Basit tasklar

#### 3. **Mistral-7B-Instruct** (BÃ¼yÃ¼k GPU)
```python
model_name="mistralai/Mistral-7B-Instruct-v0.2"
```
- **Boyut:** 7B parametreli
- **Avantaj:** YÃ¼ksek performans
- **GÃ¼Ã§lÃ¼:** Reasoning, uzun context

### Veri Analizi Ä°Ã§in

#### 1. **Llama-2-7b-chat** (Gated)
```python
model_name="meta-llama/Llama-2-7b-chat-hf"
```
- **Ã–zellik:** Ä°yi reasoning
- **Gerekli:** HuggingFace token

---

## âš¡ Performans Ä°puÃ§larÄ±

### 1. Model Cache Kullan

```python
# Her zaman aynÄ± cache dizinini kullan
provider = LocalModelProvider(
    model_name="...",
    cache_dir="./models/cache"  # Sabit dizin
)
```

### 2. Batch Processing

```python
# Birden fazla prompt iÃ§in
prompts = ["prompt1", "prompt2", "prompt3"]

for prompt in prompts:
    response = await provider.generate(prompt)
    print(response)
```

### 3. Model Unload

```python
# BaÅŸka model yÃ¼kleyeceksen Ã¶nce unload et
provider1.unload_model()

provider2 = LocalModelProvider(...)
```

### 4. GPU Memory TemizliÄŸi

```python
import torch
import gc

# Modeli unload et
provider.unload_model()

# Memory temizle
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

### 5. Optimal Generation Parameters

```python
response = await provider.generate(
    prompt="...",
    temperature=0.7,        # YaratÄ±cÄ±lÄ±k (0.0-2.0)
    max_tokens=256,         # Maksimum uzunluk
    top_p=0.95,            # Nucleus sampling
    top_k=50,              # Top-K sampling
    repetition_penalty=1.1  # Tekrar cezasÄ±
)
```

---

## ğŸ”§ Sorun Giderme

### 1. CUDA Out of Memory

**Hata:**
```
RuntimeError: CUDA out of memory
```

**Ã‡Ã¶zÃ¼m:**
```python
# 1. 4-bit quantization kullan
provider = LocalModelProvider(
    model_name="...",
    load_in_4bit=True  # En Ã¶nemli!
)

# 2. Daha kÃ¼Ã§Ã¼k model seÃ§
# 7B yerine 2-3B model kullan

# 3. Max tokens azalt
response = await provider.generate(
    prompt="...",
    max_tokens=128  # 256 yerine
)
```

### 2. Model Ä°ndirme YavaÅŸ

**Ã‡Ã¶zÃ¼m:**
```python
# 1. HF_HUB_ENABLE_HF_TRANSFER kullan
export HF_HUB_ENABLE_HF_TRANSFER=1

# 2. Proxy kullan (gerekirse)
export HF_ENDPOINT="https://hf-mirror.com"

# 3. Resume download
# Ä°ndirme otomatik devam eder
```

### 3. Gated Model EriÅŸim HatasÄ±

**Hata:**
```
Repository is gated
```

**Ã‡Ã¶zÃ¼m:**
```python
# 1. Model sayfasÄ±ndan eriÅŸim iste
# 2. HF_TOKEN ayarla
export HF_TOKEN="hf_xxxxx"

# 3. Token'Ä± kod iÃ§inde kullan
provider = LocalModelProvider(
    model_name="meta-llama/Llama-2-7b-chat-hf",
    hf_token=os.getenv("HF_TOKEN")
)
```

### 4. bitsandbytes HatasÄ±

**Hata:**
```
ImportError: bitsandbytes not found
```

**Ã‡Ã¶zÃ¼m:**
```bash
# CUDA varsa
pip install bitsandbytes

# CPU'da (quantization yok)
# load_in_4bit/8bit kullanma
```

### 5. YavaÅŸ Inference

**Ã‡Ã¶zÃ¼m:**
```python
# 1. GPU kullan
provider = LocalModelProvider(
    model_name="...",
    device="cuda"  # CPU yerine
)

# 2. KÃ¼Ã§Ã¼k model seÃ§
# 7B yerine 1-3B

# 3. Max tokens azalt
max_tokens=128  # 512 yerine

# 4. Batch size artÄ±r (training iÃ§in)
```

---

## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma

| Model | Parametreler | GPU Memory | HÄ±z | Kalite | KullanÄ±m |
|-------|-------------|-----------|-----|--------|----------|
| TinyLlama-1.1B | 1.1B | ~1GB | âš¡âš¡âš¡ | â­â­ | Basit chat |
| Phi-2 | 2.7B | ~2.5GB | âš¡âš¡âš¡ | â­â­â­â­ | Genel amaÃ§lÄ± |
| CodeLlama-7B | 7B | ~4GB (4-bit) | âš¡âš¡ | â­â­â­â­â­ | Kod Ã¼retimi |
| Mistral-7B | 7B | ~4GB (4-bit) | âš¡âš¡ | â­â­â­â­â­ | Reasoning |
| Llama-2-7B | 7B | ~4GB (4-bit) | âš¡âš¡ | â­â­â­â­ | Chat |

---

## ğŸ”— Kaynaklar

- [HuggingFace Hub](https://huggingface.co/models)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [Model Cards](https://huggingface.co/docs/hub/model-cards)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

---

## ğŸ’¡ Ã–rnek KullanÄ±mlar

DetaylÄ± Ã¶rnekler iÃ§in:
```bash
python examples/huggingface_examples.py
```

Her Ã¶rnek:
- âœ… Model indirme
- âœ… Quantization
- âœ… Agent entegrasyonu
- âœ… Performans karÅŸÄ±laÅŸtÄ±rma

---

**Son GÃ¼ncelleme:** 2025-11-16  
**Versiyon:** 1.0.0

